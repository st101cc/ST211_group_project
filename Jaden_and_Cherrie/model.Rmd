Setup
```{r}
rm(list=ls())
library(arm)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(car)
```

# Load data

cleaned data
```{r}
data<-read.csv("../data/W8QDEB2_cleaned.csv",header = TRUE, stringsAsFactors=TRUE)
head(data)
summary(data)
```
# Transformations 

transform continuous variables into categorical variables
```{r}
cont_vars <- c("W1GrssyrHH", "W1yschat1", "W2ghq12scr", "W4schatYP", "W6DebtattYP", "W8DGHQSC")
data[cont_vars] <- lapply(data[cont_vars], factor)
```

make sure binary variables are factors
```{r}
binary_vars <- c("W1hea2MP", "W1usevcHH", "IndSchool", "W1truantYP", "W1alceverYP", "W1bulrc","W5JobYP", "W5EducYP", "W5Apprent1YP", "W6JobYP", "W6UnivYP", "W6EducYP","W6Apprent1YP", "W6OwnchiDV", "W6Childliv")
data[binary_vars] <- lapply(data[binary_vars], factor)
```

The following models are obtained using backward elimination:
  
# Model 1
  
run regression on data
```{r}
model.1 <- lm(W8QDEB2 ~ ., data = data)

# have a look at the model
summary(model.1)
```
Keep predictors with more than 10% significant value in the model: "W1GrssyrHH", "IndSchool", "W1depkids", "W4RacismYP"

# Model 2

run regression with predictors having more than 10% significant value from the previous model 

```{r}
model.2 <- lm(W8QDEB2 ~ W1GrssyrHH + W1NoldBroHS + W1hiqualdad +
                      W1depkids + IndSchool + W4RacismYP + W8TENURE, data = data)

# have a look at the model
summary(model.2)
```


# Check for interactions

```{r}
# Fitting a model with interaction terms between predictors
model.interaction <- lm(W8QDEB2 ~ W1GrssyrHH * W1NoldBroHS + W1GrssyrHH * IndSchool + W1NoldBroHS * W4RacismYP + IndSchool * W4RacismYP, data = data)

# View the summary of the model with interactions
summary(model.interaction)
```

In the regression model with interaction terms that you've run, you should decide which predictors and interaction terms to keep based on their statistical significance and the explanatory value they add to the model. Here's how to interpret the results and decide which components to retain:
  
  Interpretation of the Regression Output
Significant Predictors:
  
  IndSchool1: Coefficient = 310,500, p-value = 1.11e-15 (highly significant)
IndSchool1:W4RacismYP: Coefficient = -147,500, p-value = 1.93e-14 (highly significant)
W1GrssyrHH:W1NoldBroHS: Coefficient = 0.08258, p-value = 0.0416 (significant at the 5% level)
Other Predictors:
  
  Other terms like W1GrssyrHH, W1NoldBroHS, and W4RacismYP on their own do not show significant p-values, suggesting they might not have a strong independent effect on W8QDEB2.
Decisions on Which Terms to Keep
Keep Significant Interaction Terms: IndSchool1 and W4RacismYP: This interaction term is highly significant and suggests that the effect of school independence on W8QDEB2 varies depending on the levels of perceived racism (W4RacismYP). 
This term adds important insights into how these variables interact to affect the outcome.
W1GrssyrHH and W1NoldBroHS: This term is also significant, indicating a specific interaction between gross yearly household income and the number of older brothers, which affects the dependent variable.
Review the Role of Individual Predictors:IndSchool1: Even alone, this predictor is highly significant and should definitely be retained.
For other non-significant predictors (e.g., W1GrssyrHH, W1NoldBroHS, W4RacismYP), consider whether they are necessary as control variables or if their inclusion is supported by theory. If not, they might be candidates for removal to simplify the model

# Model 4

run regression with predictors having more than 5% significant value from the previous model and include the interaction term

```{r}
model.4 <- lm(W8QDEB2 ~ W1GrssyrHH + W1NoldBroHS + W1hiqualdad +
                W1depkids + IndSchool + W4RacismYP + W8TENURE + W1GrssyrHH:W1NoldBroHS + IndSchool:W4RacismYP, data = data)

# have a look at the model
display(model.4)
summary(model.4)
```
# Summarize model 4
```{r}
par(mfrow=c(2,2))
plot(model.4, which=c(1,2))
hist(rstandard(model.4), freq = FALSE ,
     main="Histogram of standardised residuals",
     cex.main=0.8, xlab="Standardised residuals")
```
The Residual vs. Fitted plot indicates potential heteroscedasticity. 

The Normal Q-Q plot signifies that there may be potential outliers in the model. 

Histogram of standardised residuals shows that the residuals are not normally distributed. 


# Rmove outliers

Apply function from lecture

```{r}
show_outliers<-function(the.linear.model,topN){
  #length of data
  n=length(fitted(the.linear.model))
  #number of parameters estimated
  p=length(coef(the.linear.model))
  #standardised residuals over 3
  res.out<-which(abs(rstandard(the.linear.model))>3) #sometimes >2
  #topN values
  res.top<-head(rev(sort(abs(rstandard(the.linear.model)))),topN)
  #high leverage values
  lev.out<-which(lm.influence(the.linear.model)$hat>2*p/n)
  #topN values
  lev.top<-head(rev(sort(lm.influence(the.linear.model)$hat)),topN)
  #high diffits
  dffits.out<-which(dffits(the.linear.model)>2*sqrt(p/n))
  #topN values
  dffits.top<-head(rev(sort(dffits(the.linear.model))),topN)
  #Cook's over 1
  cooks.out<-which(cooks.distance(the.linear.model)>1)
  #topN cooks
  cooks.top<-head(rev(sort(cooks.distance(the.linear.model))),topN)
  #Create a list with the statistics -- cant do a data frame as different lengths 
  list.of.stats<-list(Std.res=res.out,Std.res.top=res.top, Leverage=lev.out, Leverage.top=lev.top, DFFITS=dffits.out, DFFITS.top=dffits.top, Cooks=cooks.out,Cooks.top=cooks.top)
  #return the statistics
  list.of.stats}
```

```{r}
#Get outliers
outliers <- show_outliers(model.4, topN = 5) 

# Combine all outlier indices into a single vector
all_outliers <- unique(c(outliers$Std.res, outliers$Leverage, outliers$DFFITS, outliers$Cooks))

# Remove outliers from the dataset
cleaned_data <- data[-all_outliers, ]

# Refit the model without outliers
model.5 <- lm(W8QDEB2 ~ W1GrssyrHH + W1NoldBroHS + W1hiqualdad +
                W1depkids + IndSchool + W4RacismYP + W8TENURE + W1GrssyrHH:W1NoldBroHS + IndSchool:W4RacismYP, data = cleaned_data)

# have a look at the model
display(model.5)
summary(model.5)
```
# Check colinearility
```{r}
vif(model.5)
```
The vif() function output for model.5 indicates that IndSchool and IndSchool:W4RacismYP both have VIF values greater than 7, which is not as high as the usual threshold of 10, 

# Model 6
Remove the highly collinear terms and re-fit the model
```{r}
model.6 <- lm(W8QDEB2 ~ W1GrssyrHH + W1NoldBroHS + W1hiqualdad +
                W1depkids + IndSchool + W8TENURE + W1GrssyrHH:W1NoldBroHS + IndSchool:W4RacismYP + W4RacismYP, data = data)

summary(model.6)
```
The predictors explain around 14% of the variability in "W8QDEB2".

# Calculate VIF for the revised model
```{r}
vif(model.6)
```


# Final model analysis

```{r}
par(mfrow=c(2,2))
plot(model.6,which=c(1,2))
hist(model.6$residuals,main="Histogram of residuals",font.main=1,xlab="Residuals")
```
